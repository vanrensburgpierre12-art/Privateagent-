services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama-3.2-70b}
      - CHROMA_PERSIST_DIR=/app/chroma_persist
      - EMBEDDINGS_PROVIDER=${EMBEDDINGS_PROVIDER:-sentence_transformers}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence-transformers/all-MiniLM-L6-v2}
      - MAX_CONTEXT_TOKENS=${MAX_CONTEXT_TOKENS:-4000}
      - CHUNK_SIZE=${CHUNK_SIZE:-500}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-50}
      - CORS_ORIGINS=http://localhost:5173,http://localhost:3000,http://frontend:5173,http://127.0.0.1:5173
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./data/chroma:/app/chroma_persist
      - ./backend/app:/app/app
    depends_on:
      - frontend
    networks:
      - app-network
    restart: unless-stopped

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000
    volumes:
      - ./frontend/src:/app/src
    networks:
      - app-network
    restart: unless-stopped

networks:
  app-network:
    driver: bridge

volumes:
  chroma_data:
    driver: local